{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1125,
     "status": "ok",
     "timestamp": 1746582153938,
     "user": {
      "displayName": "So Dude",
      "userId": "04035097424418043678"
     },
     "user_tz": -480
    },
    "id": "s5OcsjjTxgit"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import zipfile\n",
    "from IPython.display import display\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "import logging\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPvXo0SxRTGD"
   },
   "source": [
    "#each date_df is a continuous data from a patient, we do interpolation and return the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1746582185582,
     "user": {
      "displayName": "So Dude",
      "userId": "04035097424418043678"
     },
     "user_tz": -480
    },
    "id": "m87CrC4IeFRT"
   },
   "outputs": [],
   "source": [
    "# for each date_df do interpolation and return interpolated date_df\n",
    "def time_interpolation_of_day(date_df):\n",
    "    try:\n",
    "\n",
    "        if date_df.empty:\n",
    "            return None\n",
    "\n",
    "        # Set the 'timestamp' column as the index\n",
    "        date_df.set_index(\"time\", inplace=True)\n",
    "\n",
    "        # Resample the data to 1-minute frequency and interpolate missing values\n",
    "        date_df = date_df.resample(\"min\").interpolate()\n",
    "\n",
    "        # need to reset index or cannot filter\n",
    "        date_df = date_df.reset_index()\n",
    "\n",
    "        # check nan in gl column\n",
    "        if date_df[\"gl\"].isna().sum() > 0:\n",
    "            # remove rows with nan value\n",
    "            date_df = date_df.dropna(subset=[\"gl\"])\n",
    "\n",
    "        date_df[\"gl\"] = date_df[\"gl\"].astype(int)\n",
    "\n",
    "        # Find the entry with timestamp at midnight\n",
    "        midnight = date_df[date_df[\"time\"].dt.time == pd.Timestamp(\"00:00\").time()]\n",
    "\n",
    "        # make sure at least one day data available\n",
    "        if len(midnight) < 2:\n",
    "            return None\n",
    "\n",
    "        # Find the first midnight entry\n",
    "        first_midnight_time = midnight.iloc[0].at[\"time\"]\n",
    "\n",
    "        # Find the last midnight entry\n",
    "        last_midnight_time = midnight.iloc[-1].at[\"time\"]\n",
    "\n",
    "        # Filter the DataFrame to keep only the between first and last midnight\n",
    "        date_df = date_df[\n",
    "            (date_df[\"time\"] >= first_midnight_time)\n",
    "            & (date_df[\"time\"] < last_midnight_time)\n",
    "        ]\n",
    "\n",
    "        # --- Filter out the remainder rows from the beginning ---\n",
    "        # the following code are just left for double check\n",
    "        total_rows = len(date_df)\n",
    "\n",
    "        remainder = total_rows % 1440\n",
    "\n",
    "        if remainder != 0:\n",
    "            start_index = remainder\n",
    "            date_df = date_df.iloc[start_index:]  # Keep rows from start_index to end\n",
    "            print(f\"Removed {remainder} rows\")\n",
    "\n",
    "        # return date_df start from 00:00 and end at 11:59 with 1 min interpolation\n",
    "        return date_df\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing date_df: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6uRlTeBRsGP"
   },
   "source": [
    "#for each id(patient), chunk dataframe if there is any time difference for over 3 hours, and then call function to interpolate. After interpolation, concat possible continuous data together(with <= 1 day diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1746582193947,
     "user": {
      "displayName": "So Dude",
      "userId": "04035097424418043678"
     },
     "user_tz": -480
    },
    "id": "4tUvv_SOxHYN"
   },
   "outputs": [],
   "source": [
    "def chunk_by_time(id_group):\n",
    "    id, id_df = id_group\n",
    "\n",
    "    try:\n",
    "        # sort by time\n",
    "        id_df = id_df.sort_values(by=[\"time\"])\n",
    "\n",
    "        # Reset index\n",
    "        id_df = id_df.reset_index(drop=True)\n",
    "\n",
    "        # Create a column with date diff from the previous entry\n",
    "        id_df[\"diff_time\"] = id_df[\"time\"].diff()\n",
    "\n",
    "        # Filter out rows where 'diff_time' is greater than 3 hours\n",
    "        split_indices = id_df.index[\n",
    "            id_df[\"diff_time\"] >= pd.Timedelta(hours=3)\n",
    "        ].tolist()\n",
    "\n",
    "        # Add the first and last index to the split points\n",
    "        # it is ok if the last one and the second last one are both id_df.index[-1] because we will remove empty df\n",
    "        split_indices = [0] + split_indices + [id_df.index[-1]]\n",
    "\n",
    "        # Split the DataFrame into chunks based on the split points and make deep copies\n",
    "        chunks = [\n",
    "            id_df.iloc[split_indices[i] : split_indices[i + 1]]\n",
    "            .copy(deep=True)\n",
    "            .reset_index(drop=True)\n",
    "            for i in range(len(split_indices) - 1)\n",
    "        ]\n",
    "\n",
    "        # do interpolation to all chunks in parallel\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results = list(executor.map(time_interpolation_of_day, chunks))\n",
    "\n",
    "        # Remove None results\n",
    "        results = [e for e in results if e is not None]\n",
    "\n",
    "        # if a chunk's tail and next chunk's head timestamp is within 1 days, concat\n",
    "        i = 0\n",
    "        while i < len(results) - 1:\n",
    "            prev_tail_time = results[i].iloc[-1].at[\"time\"]\n",
    "            next_head_time = results[i + 1].iloc[0].at[\"time\"]\n",
    "            if (next_head_time - prev_tail_time).days <= 1:\n",
    "                results[i] = pd.concat([results[i], results[i + 1]])\n",
    "                # sort by time\n",
    "                results[i] = results[i].sort_values(by=[\"time\"])\n",
    "                # reset index\n",
    "                results[i].reset_index(drop=True, inplace=True)\n",
    "                results.pop(i + 1)\n",
    "            i += 1\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing id in chunk_by_time: {id}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87RWpJ6xVpmu"
   },
   "source": [
    "#save each chunk to npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1746582199697,
     "user": {
      "displayName": "So Dude",
      "userId": "04035097424418043678"
     },
     "user_tz": -480
    },
    "id": "RWe_3cd8v2pK"
   },
   "outputs": [],
   "source": [
    "from datetime import time\n",
    "\n",
    "\n",
    "# save each chunk of the result\n",
    "def save_chunk_in_result(i, chunk, trial):\n",
    "    try:\n",
    "        if chunk is None:\n",
    "            return\n",
    "        if chunk.empty:\n",
    "            return\n",
    "\n",
    "        # get first index\n",
    "        first_idx = chunk.index[0]\n",
    "\n",
    "        # get id from first index\n",
    "        id = chunk.at[first_idx, \"id\"]\n",
    "\n",
    "        # round time column to date and save unique values(date)\n",
    "        chunk_date = chunk[\"time\"].dt.date.unique()\n",
    "\n",
    "        gl_array = chunk[\"gl\"].to_numpy()\n",
    "\n",
    "        gl_array = gl_array.reshape(-1, 1440)\n",
    "\n",
    "        gl_array = np.expand_dims(\n",
    "            gl_array, axis=-1\n",
    "        )  # shape is (days, 1440(timestamp), 1(glucose)) for each chunk\n",
    "\n",
    "        folder_path = trial\n",
    "\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        file_path = os.path.join(folder_path, f\"{id}_{i}.npy\")\n",
    "\n",
    "        np.save(file_path, gl_array)\n",
    "\n",
    "        time_folder_path = os.path.join(folder_path + \"_time\")\n",
    "\n",
    "        os.makedirs(time_folder_path, exist_ok=True)\n",
    "\n",
    "        time_file_path = os.path.join(time_folder_path, f\"{id}_{i}_time.npy\")\n",
    "\n",
    "        np.save(time_file_path, chunk_date)\n",
    "\n",
    "        return\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing trial in save_chunk_in_result: {trial}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1746582210184,
     "user": {
      "displayName": "So Dude",
      "userId": "04035097424418043678"
     },
     "user_tz": -480
    },
    "id": "QpgbPCNbvVjG"
   },
   "outputs": [],
   "source": [
    "# each result is a list of chunks for each id\n",
    "def save_result_for_id(result, trial):\n",
    "    try:\n",
    "\n",
    "        if not result:\n",
    "            return\n",
    "\n",
    "        trials = [trial] * len(result)\n",
    "\n",
    "        # Enumerate over the zipped lists\n",
    "        enumerated_zipped_lists = enumerate(zip(result, trials))\n",
    "\n",
    "        # Save each chunk in result in parallel\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            executor.map(\n",
    "                lambda args: save_chunk_in_result(args[0], *args[1]),\n",
    "                enumerated_zipped_lists,\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing trial save_result_for_id: {trial}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3A_4gsOWUHW"
   },
   "source": [
    "# for each trial, group dataframe by id and then apply further function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1746582211894,
     "user": {
      "displayName": "So Dude",
      "userId": "04035097424418043678"
     },
     "user_tz": -480
    },
    "id": "OJgCuxR4ZCBv"
   },
   "outputs": [],
   "source": [
    "# for each df of different trials(for each item in df_dictionary), group by id and then time\n",
    "def chunk_id_and_time(trial, df, healthy=False):\n",
    "\n",
    "    try:\n",
    "\n",
    "        df = df[[\"id\", \"time\", \"gl\"]]\n",
    "\n",
    "        # change dtypes of time column to timestamp\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "\n",
    "        # sort by id and time\n",
    "        df = df.sort_values(by=[\"time\", \"id\"])\n",
    "\n",
    "        # round timestamp in 'time' column to minute\n",
    "        df[\"time\"] = df[\"time\"].dt.round(\"1min\")\n",
    "\n",
    "        # Remove redundant rows based on 'id' and 'time', keeping the first occurrence\n",
    "        df = df.drop_duplicates(subset=[\"id\", \"time\"], keep=\"first\")\n",
    "\n",
    "        # Group by id\n",
    "        id_group = df.groupby(\"id\")\n",
    "\n",
    "        # Process each id_group in parallel using ThreadPoolExecutor\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results = list(executor.map(chunk_by_time, id_group))\n",
    "\n",
    "        if not results:\n",
    "            return\n",
    "\n",
    "        # remove result which is empty list\n",
    "        results = [result for result in results if result]\n",
    "\n",
    "        \"\"\"\n",
    "    although there is large intervals between each chunk in results\n",
    "    the pattern would not change much in healthy subjects\n",
    "    it is ok to concatenate them together\n",
    "    and generate numpy array with shape(days, 1440(timestamp))\n",
    "    \"\"\"\n",
    "        \"\"\"\n",
    "    but in t1 patient, different chunks should be seperated\n",
    "    \"\"\"\n",
    "        # if healthy, concatenate df in each result together\n",
    "        if healthy:\n",
    "\n",
    "            results = [[pd.concat(result).reset_index(drop=True)] for result in results]\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "\n",
    "            executor.map(save_result_for_id, results, [trial] * len(results))\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing trial in chunk_id_and_time: {trial}: {e}\")\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPTqrDDSHz/abdjLInOFOR5",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
